{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [

  
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students : \n",
    "    \n",
    "## Sai Teja Pothnak , Student ID : 30326460\n",
 
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Kaggle Name Used for group Submission : Group26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The science of classifying the different writing styles of authors by analyzing different characteristics of work and closely examining the articles is Authorship Analysis. It measures some textual features and aims to determine some characteristics like - age, gender, native language and other personality traits based on the textual data we have. <br>\n",
    "Authorship Analysis is increasing its popularity with increasing applications in the field of Forensic, Crime investigations, Plagarism softwares etc. Authorship Analysis can be classified into following-\n",
    "1. Authorship Attribution - The odds of a particular article written by particular author. We can determine the writer of the document by this technique.\n",
    "2. Authorship Profiling - Analysing the articles(text) for uncovering different aspects/charecteristics of the author. eg Finding gender, age group etc\n",
    "3. Similarity Detection - Analysis of how much is the actual contribution of the author to a particular published article. Checking the Authenticity of the text. eg - Plagiarism Softwares. \n",
    "\n",
    "In the given task we are asked to perform Author Profiling task where we are asked to predict the gender of the Author based on his work/article. This is considered to be a Classification problem where we will classify the genders as either \"Male\" or \"Female\" by first performing NLP tasks (Pre Processing tasks) on the text and extracting features from the data in vectorized form and then passing data to the classifier to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AIM** - Develop classifier that can identify the gender of the tweet's author as accurate as possible.<br>\n",
    "**About Data** - we are given the following datasets:\n",
    "1. train_labels.csv - Data for training our classifoer containing twitter Posts of 3100 authors.\n",
    "2. test.csv - This has only Author_id's for which we have to predict the gender.\n",
    "3. data.zip - This has 3600 twitter texts which we will use for training and testing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile   ## For reading/writing Zip Files\n",
    "import zipfile\n",
    "import re                     ## For using regular expressions\n",
    "import pandas as pd           ## For using Data structures for Data Manipulation\n",
    "import os\n",
    "from nltk.corpus import stopwords              ## For removing stopwords\n",
    "from nltk import word_tokenize                 ## For tokenizing the text\n",
    "from nltk.stem import WordNetLemmatizer        ## Lemmatizing the text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer   ## For Vectorizing the text\n",
    "## For Calculating and displaying Metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier    ## Using the Logistic Regression model of sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB,BernoulliNB\n",
    "from sklearn.svm import LinearSVC, SVC         ## Using the Linear SVC algorithm of Sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier    ## Using the Random Forest Classifier of sklearn\n",
    "#from sklearn import cross_validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV     ## For Hyper Parameter tuning\n",
    "import numpy as np         ## For  Scientific Calculations\n",
    "import seaborn as sns      ## For visualizations\n",
    "import matplotlib.pyplot as plt  ## For Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Data from the Zipfiles and Reading the Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the Folder from the Zip File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting all file...\n",
      "Extracting Done!\n"
     ]
    }
   ],
   "source": [
    "my_zipfile = ZipFile(\"data.zip\", mode='r')\n",
    "\n",
    "print('Extracting all file...')\n",
    "\n",
    "my_zipfile.extractall()\n",
    "\n",
    "print('Extracting Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data Files \n",
    "\n",
    "# A dictionary will be created to store each id and its correspoding chunk from below obtained lists.\n",
    "## Cleaning is performed in the chunks(tweet) list obtained below and then a dictionary will be created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list =[]                          ## List for storing the ID's \n",
    "tweets =[]                           ## List for storing the actual tweet textual data\n",
    "for filename in os.listdir('data/'):\n",
    "    if filename.endswith('xml'):     ## checking for xml files\n",
    "        tweets.append(open('data/'+filename,encoding='utf-8').read())  ## appending text to the tweets list\n",
    "        id_list.append(filename.strip('.xml'))   ## Appending the ID's to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Text Preprocessing?**<br>\n",
    "Text preprocessing is the first step in the pipeline of Classification using Natural Language Processing system. The way we preprocess the data has a huge impact on the final output of our model.<br>\n",
    "It is basically performing some Cleaning tasks on the huge textual data so as to make it predictable and analyzable for our task. As we have data in Human Readable format, for further analysis and processing by our Algorithms we need the data in Machine Readable Format. There are multiple ways to preprocess our data. Following are the approaches to do the same. <br>\n",
    "**Text Normalization** -<br>\n",
    "    - Converting the data into one case. That is converting all the text in Lowercase/Uppercase. \n",
    "    - Removing Numbers as they are not relevant in our context .\n",
    "    - Removing the Punctuations.\n",
    "    - Removing all the white spaces.\n",
    "    - Taking care of the abbreviations.\n",
    "    - Removal of emoticons,map symbols, hyperlinks, pictographs etc <br>\n",
    "**Tokenization**-<br>\n",
    "Here we split the large text into small pieces called as Tokens. It breaks a paragraph in sentences, sentences into individual stream of words. <br>\n",
    "After Tokenizing the text the next important step is to remove words which are commonly used and usually do not carry any meaning with them.Such words are called as **stop words**.Libraries like NLTK already have a corpus which stores the stop words in it. Removing them before generating features will reduce the number of features being generated which are not much useful which results in a decent size model.Removing them before passing on the data to further steps would hugely increase the accuracy of our classifier.<br>\n",
    "As we have an XML file, we would like to remove the tags if any in the file as they dont add any value to our text. We will expand words like **I'm** with **I am** in our text.<br>\n",
    "We have created a one single function which would perform all the cleaning tasks at once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning Tasks :\n",
    "1. Removal of empty spaces and spaces created by tab press.\n",
    "2. As we have XML data, removing the XML tags is one of the task.\n",
    "3. Removing the Hyperlinks from the text.\n",
    "4. Removing the emoticons, Symbols, pictographs, map symbols etc.\n",
    "5. Expanded the abbreviated words/Phrases.\n",
    "6. Converted the Text into lowercase.\n",
    "7. Converted the Text into Tokens.\n",
    "8. Considered only alphabetic tokens as rest are irrelevant for our task.\n",
    "9. Removal of stop words.\n",
    "10. Joining the tokens back to achieve clean string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tweet):\n",
    "    # tweet\n",
    "    tweet=re.sub('\\n','',tweet)                                  ## Removing the \\n from the file\n",
    "\n",
    "    tweet=re.sub('\\t','',tweet)                                  ## Removing the empty tab spaces from the file\n",
    "    \n",
    "    ## Substitute the xml tags from the file\n",
    "\n",
    "    tweet=re.sub('<documents>','',tweet)                         \n",
    "\n",
    "    tweet=re.sub('</documents>','',tweet)\n",
    "\n",
    "    tweet=re.sub('<document>','',tweet)\n",
    "\n",
    "    tweet=re.sub('</document>','',tweet)\n",
    "\n",
    "    tweet=re.sub('!\\[CDATA\\[','',tweet)\n",
    "\n",
    "    tweet=re.sub('<author lang=\"en\"','',tweet)\n",
    "\n",
    "    tweet=re.sub('></author>','',tweet)\n",
    "\n",
    "    tweet=re.sub('><','',tweet)\n",
    "\n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')  ## pattern to find the hyperlinks\n",
    "    tweet=pattern.sub('',tweet)       ## Substituting the hyperlinks with empty string. \n",
    "    tweet=re.sub('@','',tweet)        #substituting @ with empty string \n",
    "    \n",
    "    emoji = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)      \n",
    "    \n",
    "    tweet=emoji.sub(r'',tweet)                       # Substituting the emojis, pictographs, transport & map symbols, flags by empty string\n",
    "    tweet=re.sub('&amp;','',tweet)                   #Substituting amp with empty string\n",
    "    ## Substituting all faulty words with their proper expanded version.\n",
    "    tweet = re.sub(r\"i'm\", \"i am\", tweet)\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"she's\", \"she is\", tweet)\n",
    "    tweet = re.sub(r\"that's\", \"that is\", tweet)        \n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet) \n",
    "    tweet = re.sub(r\"\\'ll\", \" will\", tweet)  \n",
    "    tweet = re.sub(r\"\\'ve\", \" have\", tweet)  \n",
    "    tweet = re.sub(r\"\\'re\", \" are\", tweet)\n",
    "    tweet = re.sub(r\"\\'d\", \" would\", tweet)\n",
    "    tweet = re.sub(r\"\\'ve\", \" have\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"can not\", tweet)\n",
    "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet=re.sub(r\"i’ve\",'i have',tweet)\n",
    "    tweet=re.sub(r\"i’d\",'I would',tweet)\n",
    "    \n",
    "    tweet = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \"\", tweet)    ## Removal of Punctuations from the text\n",
    "    \n",
    "    tweet=tweet.lower()                                              ## converting the tweet text to lowercase\n",
    "    \n",
    "    tweet=re.sub('\\]\\]',' ',tweet)\n",
    "    \n",
    "    wordtokens=word_tokenize(tweet)                                  ## Converting the Text into small tokens\n",
    "    \n",
    "    words=[x for x in wordtokens if x.isalpha()]                     ## Removing Non alphabetic data from the tokens if any.\n",
    "    #remove stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    \n",
    "    words=[x for x in words if x not in stop_words]                  ## appending only those words which dont fall in Stop Words list\n",
    "\n",
    "    clean_tokens=(' '.join(words))                                   ## Appending the tokens for each tweet text into one single text by joining it with ''\n",
    "\n",
    "    \n",
    "    \n",
    "    return (clean_tokens)                                            ## Returning the clean text    \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing cleaning on tweet chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data=[]\n",
    "for i in tweets:\n",
    "    \n",
    "    clean_data.append(clean_text(i))     ## appending the clean text in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dictionary that has id from  and its corresponding chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dict(zip(id_list, clean_data)) #zipping the idlist anf clean data and forming a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEMMATIZATION\n",
    "When we have a huge amount of textual data with us, there are bound to be words which have same meaning but they come in different forms. For example - car, cars etc. Lemmatization will reduce such words to their base form or we can say root word. It removes the inflectional endings to the root form. It makes use of vocabulary and morphological analysis of the words for performing the task. It has a detailed dictionary which it will look up before giving us a lemma for any word. The Lemma is the base of all the inflectional forms.<br>\n",
    "We have used WordNetLemmatizer for our task. It has a dictionary which has over 50 corpuses and lexical resources which it refers for returning the base form of the word. It will return the input word if it dosent find in the WordNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object): #class object\n",
    "    def __init__(self):\n",
    "        self.wnl=WordNetLemmatizer() #initialising wordnet lemmatizer\n",
    "    def __call__(self,doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)] #to perform lemmatization on words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Vectorizer\n",
    "Before we actually vectorize/encode the tokens we have, its important to check the frequency of the words in the documents. If we perform simple counts of the words in the document, it is possible that some words appear alot of times in a single document. Hence we make use of TFIDf vectorizer. TF-IDF stands for **Term Frequency Inverse Document Frequency**. It will first calculate the occurences of a word in a single document and then will find the frequency of the same word in other documents, If it finds that the word has many occurences in other documents as well, It will downscale the weighatge of the word. This is called as the Inverse Document Frequencies. We can say that it will give us words frequent in a document but not across documents.<br>\n",
    "We have set some of the parameters below so as to improve its performance.<br>\n",
    "    - Converted all words to lowercase.\n",
    "    - Used lemmaTokenizer for tokenizing the text.\n",
    "    - ngram range - (1,3) which means we are interested in extracting unigrams,bigrams,trigrams from our text.\n",
    "    - norm = l2. Sum of squares of vector elements is 1. The cosine similarity between two vectors is their dot product when l2 norm has been applied.\n",
    "**Bigrams** - Bigrams are basically a pair of two words. There might be some words in the document which makes more sense when they are together. For example - **Machine Learning**.<br>\n",
    "**Trigrams** - Phrases having three words together.<br>\n",
    "Ngrams will give us words/Phrases that are often used together in the textual documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(analyzer='word',input='content', \n",
    "                           lowercase=True,                   ## Converting strings to lowercase\n",
    "                           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',  ## token pattern to be searched in the text\n",
    "                           min_df=1,                           ## Minimum document frequency\n",
    "#                            stop_words=set(stop),\n",
    "                           ngram_range=(1,3),                 ## Ngrams range \n",
    "                           norm='l2',                         ## regularization term\n",
    "                           tokenizer=LemmaTokenizer())        ## Tokenizer used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methodology for getting Data Ready before applying the Models on Our Data :\n",
    "1. Reading the Training and Test data using Pandas Dataframes.\n",
    "2. Convert the ID's and the Labels of Training Data into List format.\n",
    "3. Creating List that would store the Clean chunk of tweet with respect to each ID . We already stored this information in a dictionary which has ID's and their corresponding clean tweets.\n",
    "4. Creating a number representation of the Gender Class Labels.\n",
    "5. Vectorizing the text using Fit-Transform.\n",
    "6. Applying 5-fold Cross Validation on training models and then plotting their accuracies for comparing them.\n",
    "7. Creating the final Test Data dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the datafiles using Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData=pd.read_csv('train_labels.csv')  ## creating the dataframe of training data \n",
    "testData=pd.read_csv('test.csv')  ## creating the dataframe of testing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### storing trainData ids and labels as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=trainData.id.tolist()            ## creating a list of ID's from training data\n",
    "labels=trainData.gender.tolist()     ##  taking the genders into a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a list to store chunks of each train id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chunk=[]                       ## List that will store Chunks of text for Training Data\n",
    "for i in ids:\n",
    "    train_chunk.append(dictionary[i])    ## appending the tweet text into a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a list to store number converted representation for each gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=[]                                 ## The list that will store Train Labels\n",
    "for i in labels:\n",
    "    if i =='male':\n",
    "        y_train.append(1)                 ## Appending '1' if the label is Male and 0 Otherwise.\n",
    "    else:\n",
    "        y_train.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorising the text and storing the labels as array to train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_transform : It applies to feature extraction objects. Fit part will determine what features the model will base its future transformations on. The transform function gives us the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=vectorizer.fit_transform(train_chunk)   ## Applying fit transform function on our text  \n",
    "Ytrain=np.asarray(y_train)          ## converting the labels in array format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the accuracies of different models using cross validation using a box plot for comparing them. We are performing the CV on our Training Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHRUTI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\SHRUTI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\SHRUTI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\SHRUTI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_1 = LogisticRegression()              ## creating a Model for Logistic Regression\n",
    "clf_2 = SGDClassifier()          ## Model for Random Forest Classifier\n",
    "clf_3 = LinearSVC()                       ## Model for Linear SVC\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    LinearSVC(),\n",
    "    SGDClassifier(),\n",
    "    VotingClassifier(estimators=[('LR', clf_1), ('SGD', clf_2), ('LSVC', clf_3)], voting='hard')\n",
    "    \n",
    "]            \n",
    "CV = 5          ## Cross Validation parameter\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))      ## Creating dataframe for storing the model name along with its accuracies\n",
    "entries = []\n",
    "for model in models:\n",
    "     model_name = model.__class__.__name__\n",
    "     accuracies = cross_val_score(model, Xtrain, Ytrain, scoring='accuracy', cv=CV)\n",
    "     for fold_idx, accuracy in enumerate(accuracies):\n",
    "          entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "#cv_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Accuracies of the Models using sns and matplotlib libraries.\n",
    "- X axis will show us the Name of the Model (Logistic Regression, Linear SVC, Random Forest Classifier, Voting Classifier) respectively.\n",
    "- y axis will show us the accuracy box plot of the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)       \n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, we can see that Linear SVC and Voting CLassifier works better on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=pd.read_csv('test_labels.csv')   ## Reading file using Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d6b08022cdf758ead05e1c266649c393</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9a989cb04766d5a89a65e8912d448328</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2a1053a059d58fbafd3e782a8f7972c0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6032537900368aca3d1546bd71ecabd1</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d191280655be8108ec9928398ff5b563</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  gender\n",
       "0  d6b08022cdf758ead05e1c266649c393    male\n",
       "1  9a989cb04766d5a89a65e8912d448328  female\n",
       "2  2a1053a059d58fbafd3e782a8f7972c0    male\n",
       "3  6032537900368aca3d1546bd71ecabd1    male\n",
       "4  d191280655be8108ec9928398ff5b563    male"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()            ## peek at the Output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id=output['id'].tolist() #storing test ids as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual=output['gender'].tolist() # storing the True Labels/genders as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching the corresponding chunks of each test id from the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunk=[]\n",
    "for i in test_id:\n",
    "    test_chunk.append(dictionary[i])       ## Appending the tweet chunks from the dictionary we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorising the test chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest=vectorizer.transform(test_chunk)   ## Applying Transform function on the Test data chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertuning the parameters : \n",
    "\n",
    "Hyper parameters are the parameters that are not directly learnt within the estimator and are indeed passed as arguments for the model building. In case of LinearSVC() hyper parameters are : 'C','penalty','loss'\n",
    "\n",
    "Hyper parameters for a model can be tuned to get best model by calculating cross-validation score for each argument setting.\n",
    "\n",
    "One of the sampling search provided in scikit learn is GridSearch CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV :\n",
    "GridSearchCV is a class from sklearn.model_selection. It is an exhaustive search over the specified parameter values for a given model/estimator.\n",
    "\n",
    "GridSearchCV implements a fit method, score method. It also implements other methods such as 'predict','transform'.\n",
    "\n",
    "The parameters used to apply above methods are optimised by cross-validated grid search.\n",
    "\n",
    "GridSearchCV instance implements on a given model i.e. fitting the model on a given dataset and evaluating all possible combinations of parameter values  and a best combination is retained.\n",
    "\n",
    "Parameters of Gridsearch CV :\n",
    "\n",
    "`estimator` : This is an estimator object such as a machine learning model like : linearSVC(). This is assumed to implement scikit learn estimator interface.\n",
    "\n",
    "`param_grid` : This is a dictionary that contains parameter names as keys and their corresponding list of parameter settings values of an estimator.\n",
    "Enables searchng over given settings.\n",
    "\n",
    "`cv` : This determines the cross-validation splitting strategy. Usually given as an input value. In case of classifier problem. Stratified K- Fold is used.\n",
    "\n",
    "`verbose` : For verbosity\n",
    "\n",
    "`n_jobs` : Jobs to run in parallel. If `n_jobs=-1`,  all processors are used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression :\n",
    "\n",
    "It is a classifier Algorithm in which the response variable is a categorical variable. Its basic idea is to find the relationship between features and the probability of an outcome. In our case, the output response varibale has two classes i.e. \"Male\" or \"Female\". This type of problem is Binomial Logistic Regression.\n",
    "\n",
    "sklearn's Logistic Regression uses regularized logistic regression using 'liblinear' library and solvers such as 'sag','lbfgs'\n",
    "\n",
    "Parameters :\n",
    "\n",
    "`penalty` : It is used to specify the norm used in the penalisation. L1-norm for feature selection and L2- norm to reduce overfitting.\n",
    "\n",
    "\n",
    " `C` : This is the inverse of regularisation strength. Higher the C value stronger is the regularisation. \n",
    "\n",
    "`solver` : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’ . Solver is an algorithm used in optimisation.\n",
    "In case of large datasets 'sag','saga ' are good choices. In smaller datasets, 'liblinear' is the better choice. \n",
    "\n",
    "\n",
    "`max-iter` : This is the number of iterations taken for a solver to converge\n",
    "\n",
    "`multi_class`:  As our case is a binary problem, this is choosen as option 'over'\n",
    "\n",
    "`verbose` : For verbosity\n",
    "\n",
    "`n_jobs` : number of CPU cores used when paralellising over classes.!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Grid CV for finding the best value of C (Regularization Parameter) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finding the best C value by giving a List of values to our Grid to check from . The range we have given is - (1,12). The object will try different combinations of C values along with the other parameters that we have set to find the Best C value that will help us improve our accuracy. If we provide tiny values of C it will give us more misclassified examples, whereas higher the C value, more the overfitting of the model on the data. Hence finding the right value of C by performing GridCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = [{'penalty' : ['l2'],'C' : np.arange(1,12,1),'solver' : ['liblinear']},]\n",
    "\n",
    "# # Create grid search object\n",
    "# clf1 = GridSearchCV(LogisticRegression(), param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "\n",
    "# # Fit on data\n",
    "# best_logistic = clf1.fit(Xtrain, Ytrain)\n",
    "# print('The best parameters of Logistic Regression are :',best_logistic.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model with best parameters on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bestlogistic = best_logistic.best_estimator_        ## Taking the model with best estimators.\n",
    "# bestlogistic.fit(Xtrain,Ytrain)                     ## Fitting the model on our training data\n",
    "\n",
    "# print('Accuracy of Logistic Regression with best parameters on train data is :',bestlogistic.score(Xtrain,Ytrain))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing Predictions and finding accuracy on Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Doing Predictions\n",
    "\n",
    "# predictions1=bestlogistic.predict(Xtest)    ## Predictions using the Best Logistic Model\n",
    "\n",
    "\n",
    "# # Storing final predictions as strings\n",
    "# final_predictions1=[]\n",
    "# for i in range(len(predictions1)):\n",
    "#     if predictions1[i]==1:\n",
    "#         final_predictions1.append('male')        ## Appending \"Male\" if the prediction is '1' and '0' otherwise\n",
    "#     else:\n",
    "#         final_predictions1.append('female')\n",
    "\n",
    "# accuracy1=accuracy_score(actual,final_predictions1)     ## Calculating Accuracy of the Model\n",
    "\n",
    "# print('Accuracy Of Logistic Regression Model with best parameters on test data is : '+ str(accuracy1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVC :\n",
    "In Linear SVC, a hyper plane is constructed in such a way that it could be worthwhile to misclassify few training observations in order to do a better job in classifying the remaining observations.\n",
    "\n",
    "Rather than seeking the largest possible margin so that every observation is not only on the correct side of hyperplane but also on the correct side of the margin, we instead allow some observations to be on the incorrect side of the margin, or even the incorrect side. \n",
    "\n",
    "\n",
    "In this, hyperplane is chosen to separate most of the training observations into two classes but may misclassify few observations.\n",
    "\n",
    "In SVC, an observation that lies strictly on the correct side of the margin does not effect the classifier. But observations that lie on the margin or on wrong side of the classifier do effect SVC.\n",
    "\n",
    "When tuning parameter C is large, then margin is large thus many observations in it determine the hyperplane.\n",
    "\n",
    "If support vectors are more in number then low variance but potentially high bias. \n",
    "\n",
    "Similarly if C is small, then classifier has low bias and high variance.\n",
    "\n",
    "Sklearn.svm.LinearSVC :\n",
    "It is SVC with kernel set as 'linear' but the optimising algorithm is 'liblinear' rather than libsvm as liblinear is more flexible in choice of penalties and loss fucntions.\n",
    "\n",
    "LinearSVC() supports both dense and sparse inputs. \n",
    "\n",
    "Parameters :\n",
    "\n",
    "`penalty` : It is used to specify the norm used in the penalisation. L1-norm for feature selection and L2- norm to reduce overfitting.\n",
    "\n",
    "`loss` : Its the loss function. Standard SVM loss is 'hinge'\n",
    "\n",
    "`C` : This is the inverse of regularisation strength. Higher the C value stronger is the regularisation.\n",
    "\n",
    "`multi_class`:  As our case is a binary problem, this is choosen as option 'over'\n",
    "\n",
    "`verbose` : For verbosity\n",
    "\n",
    "`max-iter` : This is the number of iterations taken for a solver to converge\n",
    "\n",
    "`random_state` : This controls the pseudo random number generation for shuffling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of Linear SVC , the lower the C value, the classifier has **low bias and high variance** and thus will overfit the data. As the C value increases, margin becomes larger thereby resulting in low variance and help achieve a generalized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gridsearch to determine the value of C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters of Linear SVC are : {'C': 9}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C':np.arange(1,10,1)}  ## giving the range of C values\n",
    "best_linear = GridSearchCV(LinearSVC(),param_grid,cv=3,return_train_score=True)  ## Performing the grid Cv\n",
    "best_linear.fit(Xtrain,Ytrain)   ## Fitting model on our Train Data\n",
    "print('The best parameters of Linear SVC are :',best_linear.best_params_)   ## Printing the best estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model with best parameters on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Of Linear SVC with best parameters on train data is : 0.9996774193548387\n"
     ]
    }
   ],
   "source": [
    "bestlinearSVC = best_linear.best_estimator_     ## Taking the model with best estimators.\n",
    "bestlinearSVC.fit(Xtrain,Ytrain)  ## Fitting the best model on our train data.\n",
    "\n",
    "print('Accuracy Of Linear SVC with best parameters on train data is :',bestlinearSVC.score(Xtrain,Ytrain))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing Predictions and finding accuracy on LinearSVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Of Linear SVC Model with best parameters on test data is : 0.822\n"
     ]
    }
   ],
   "source": [
    "predictions2 =bestlinearSVC.predict(Xtest)   ## Making Predictions using our best model\n",
    "\n",
    "final_predictions2=[]                        ## List that will store final predictions.\n",
    "for i in range(len(predictions2)):\n",
    "    if predictions2[i]==1:\n",
    "        final_predictions2.append('male')    ## Appending \"Male\" if the prediction is '1' and '0' otherwise\n",
    "    else:\n",
    "        final_predictions2.append('female')\n",
    "\n",
    "accuracy2=accuracy_score(actual,final_predictions2)  ## Calculating the accuracy of the model.\n",
    "\n",
    "print('Accuracy Of Linear SVC Model with best parameters on test data is : '+ str(accuracy2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emsemble Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble models are generated by combining multiple base models in order to improve the prediction performance.\n",
    "\n",
    "One of the most famous emsemble models is Random Forest CLassifier model which combines multiple decision trees, aggregates their prediction values using majority vote.\n",
    "\n",
    "In a similar manner, Voting Classifier estimates multiple individual models and then uses voting method to combine individual predictions and obtain a final prediction.\n",
    "\n",
    "Unlike Random forest, individual models of Voting Classifier need not be homogenous models(Decision Trees in Random Forest) annd can be trained with different base models like LogisticCLassifier, LinearSVC, Random Forest then use VOtingClassifier to combine the results.\n",
    "\n",
    "There are two types of Voting in Voting CLassifier:\n",
    "\n",
    "**HARD :**\n",
    "In this case, the Voting classifier chooses the class that occurs maximum among the base models. The final  prediction is made by majority vote.\n",
    "\n",
    "**SOFT :**\n",
    "In this case, class prediction is made based on average probability that is calculated using all base models. For example, base model 1 predicts positive class with 80% probability, base model 2 predicts positive class with 10% probability, Voting classifier calculates that there is 45% chance tht observation belongs to positive class and hence chooses it as negative class as there is 55% chance for a egative class.\n",
    "\n",
    "In summary, this Voting Ensemble Classifier results in better overall score by aggregating the predictions by individual base models  and covers the potential weakness by individal models.\n",
    "\n",
    "Its parameters :\n",
    "\n",
    "`estimator` : list of ('model name', model) tuples. Invoking `fit` on VotingClassifier will fit data on each model from the list.\n",
    "\n",
    "`voting` : can chose either `hard` or `soft` depending on the recommended classifier.\n",
    "\n",
    "`n_jobs` : Number of jobs to run in parallel.\n",
    "\n",
    "`verbose` : For verbosity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, base models chosen are :\n",
    "    1. Logistic Regression\n",
    "    2. Linear SVC\n",
    "    3. SGD Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the final model taking best parameters for Logistic Regression Classifier, Linear SVC and default parameters for SGD CLassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Logistic Regression, Best Parameters achieved are : {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "    \n",
    "#### For LinearSVC , Best parameters achieved are : {'C': 9}\n",
    "    \n",
    "#### The best parameters achieved from the above two models are used for building ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = LogisticRegression(C= 10, penalty ='l2', solver= 'liblinear')     ## Creating Logistic Regression Model\n",
    "# model2 = LinearSVC(C=9)                                 ## Creating Linear SVC Model\n",
    "# model3 = SGDClassifier()                             ## Creating SGD Classifier\n",
    "\n",
    "# ensemble_clf=VotingClassifier(estimators=[('lr', model1), ('svc', model2),('sgd',model3)], voting='hard')  ## voting classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble_clf.fit(Xtrain,Ytrain)            ## Fitting Ensemble Model on our Train Data\n",
    "# predictions3=ensemble_clf.predict(Xtest)   ## Predicting Labels using our ensemble model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Doing Predictions and finding accuracy on Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_predictions3=[]                   ## List to store the final predictions\n",
    "# for i in range(len(predictions3)):\n",
    "#     if predictions3[i]==1:\n",
    "#         final_predictions3.append('male')    ## Appending \"Male\" if the prediction is '1' and '0' otherwise\n",
    "#     else:\n",
    "#         final_predictions3.append('female')\n",
    "\n",
    "# accuracy3=accuracy_score(actual,final_predictions3)   ## Calculating the Accuracy of our Best Model.\n",
    "\n",
    "# print('Accuracy Of Ensemble Model on test data is : '+ str(accuracy3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating a dataframe to store test ids and their corresponding gender predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'id':test_id,'gender':final_predictions2}) #creating a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing the dataframe into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('pred_labels.csv',index=False) # writing as CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With increasing popularity of Authorship Analysis in various application areas, Authorship profiling is one of the challenging task. Preprocessing of the textual data which is nonstandardized and usually informal is the primary task. Cleaning the text is an first important task to carry out for text analysis. We have developed a function that takes care of cleaning the text right from converting to lowercase, handling emoticons,abbreviations and removing all irrelevant text including stopwords and achieving a clean text. The task given to us aims at predicting the gender of the author by looking at the tweets posted by him/her. We have used lemmatization and TFIDF vectorizer for generating features and proposed the use of the following Classifiers - LinearSVC, Logistic Regression, Voting Classifier. We have build the models on train data and then compared the performance of these models. In the next step, we have built each model on train data, made predictions on test data and calculated accuracy using the original test labels. For improving the accuracy of the classifiers, we performed GridSearchCV which gave us the best parameters of the Models. After using these parameters we got better results/predictions of the Labels(gender). Further this, we created an ensemble classifier which is a combination of LinearSVC, Logistic Regression, SGD Classifier which further boosted our accuracy to 0.816. Among all, Linear SVC performed best on our data with accuracy of 0.822. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/library/re.html\n",
    " \n",
    "https://scikitlearn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    " \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    " \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    " \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    " \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
